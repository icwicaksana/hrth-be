***

```markdown
# PRODUCT REQUIREMENTS DOCUMENT (PRD) - HR TOOL HELPER

## 1. PROJECT OVERVIEW
**Name:** HR Tool Helper (Internal App)
**Goal:** A modular "Toolbox" web application for HR staff to automate tasks using AI. Similar to "iLovePDF" but for HR processes.
**Architecture:**
*   **Frontend:** Next.js (App Router, Tailwind CSS).
*   **Backend:** FastAPI (Python).
*   **Database & Auth:** Supabase (PostgreSQL, GoTrue).
*   **Storage:** Supabase Storage (Bucket: `hr-files`).
*   **AI/LLM:** LangChain (Google Gen AI), Whisper (STT).

---

## 2. TECH STACK & LIBRARIES
*   **Backend (Python):**
    *   `fastapi`, `uvicorn`: Web Server.
    *   `supabase`: Supabase Client for DB & Storage operations.
    *   `langchain`, `langchain-google-genai`: LLM Orchestration.
    *   `python-docx`: For generating/editing MS Word documents.
    *   `ffmpeg-python`: For audio/video conversion.
    *   `pydantic`: Data validation.
*   **Database:** PostgreSQL (Managed by Supabase).
*   **Environment:** Dockerized deployment (AWS/GCP).

---

## 3. DATABASE SCHEMA (SOURCE OF TRUTH)
**CRITICAL:** The database schema is ALREADY deployed. Do not attempt to create new migrations for these tables unless specified. Use the existing schema below:

```sql
-- 1. PROFILES (Linked to Auth)
create table public.profiles (
  id uuid references auth.users not null primary key,
  email text,
  full_name text,
  role text default 'user', -- 'admin' or 'user'
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- 2. JOB POSITIONS (Master Data)
create table public.job_positions (
  id bigint generated by default as identity primary key,
  title text not null,
  criteria_text text, -- Prompt context for LLM
  created_by uuid references public.profiles(id),
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);

-- 3. ACTIVITY LOGS (History & Cost Tracking)
create table public.activity_logs (
  id uuid default gen_random_uuid() primary key,
  user_id uuid references public.profiles(id),
  tool_type text not null, -- Enum: 'cv_analyzer', 'bg_check', 'onboarding', 'interview'
  input_files text[], -- Array of Supabase Storage Paths
  output_files text[], -- Array of Generated File Paths
  result_json jsonb, -- AI Analysis Result
  cost_usd numeric(10, 6) default 0, -- Estimated Cost
  token_usage jsonb, -- e.g. {"prompt": 100, "completion": 50}
  created_at timestamp with time zone default timezone('utc'::text, now()) not null
);
```

**Storage Bucket:** `hr-files` (Private).
**Storage Policy:** Authenticated users can Insert/Select/Delete.

---

## 4. ENVIRONMENT VARIABLES (.env)
Backend must rely on these variables. DO NOT store keys in the database.
```env
SUPABASE_URL=...
SUPABASE_SERVICE_ROLE_KEY=...  # Use this for Backend operations (Bypass RLS)
GEMINI_API_KEY=...             # Or GEMINI_API_KEY
GROQ_API_KEY=...               # Optional for fast Whisper
PDDIKTI_API_URL=...            # Optional
```

---

## 5. FEATURE REQUIREMENTS & LOGIC FLOW

### **CORE REQUIREMENT: Cost & Usage Tracking**
Every tool execution must calculate the estimated cost based on token usage/audio duration and save it to `activity_logs.cost_usd` and `activity_logs.token_usage`.

### **TOOL A: CV Analyzer**
*   **Input:**
    *   PDF File (CV).
    *   `job_position_id` (Selected from DB `job_positions`).
*   **Process:**
    1.  Backend retrieves `criteria_text` from `job_positions` table.
    2.  Use LLM Vision (or robust PDF extractor) to read the CV.
    3.  LLM compares CV data vs Job Criteria.
*   **Output (JSON):** Score (0-100), Match Analysis (Boolean), Strengths (List), Weaknesses (List), Summary.
*   **Storage:** Save PDF to `hr-files`.

### **TOOL B: Background Screening (Auto-fill Doc)**
*   **Input:** Files: KTP, Ijazah, SKCK (Images/PDF) + Manual Form Data (Name, NIK, Position).
*   **Process:**
    1.  **Extraction:** LLM extracts data from uploaded files (Nomor Ijazah, University Name, SKCK Status).
    2.  **Validation:** Call PDDIKTI API (if available) to verify Ijazah.
        *   *If API Fail/Not Found:* Mark field as "Unverified" and provide Link to PDDIKTI web.
    3.  **Return Data:** Return JSON to Frontend for HR Review/Correction.
    4.  **Generation:** After HR approves/edits JSON -> Backend populates a `.docx` template using `python-docx`.
*   **Output:** Generated `.docx` file link.

### **TOOL C: Onboarding Doc Filler**
*   **Input:**
    *   Files: KTP, CV.
    *   Manual Input: Dept, Position, Join Date (Selected by HR).
*   **Process:**
    1.  LLM extracts Address, Email, Phone from KTP/CV.
    2.  Return extracted JSON to Frontend for confirmation.
    3.  After confirmation, Backend generates Onboarding Document (Docx/PDF).
*   **Output:** Generated File.

### **TOOL D: Interview Analyzer (Video/Audio)**
**CRITICAL CONSTRAINT:** Storage Optimization (Free Plan Limit 1GB).
*   **Input:** Video (`.mp4`, etc) or Audio file.
*   **Flow:**
    1.  **Frontend:** Uploads file directly to Supabase Storage `hr-files`.
    2.  **Backend:**
        *   Downloads the video file.
        *   **Converts** video to audio (`.mp3` low bitrate) using `ffmpeg`.
        *   **IMMEDIATELY DELETES** the original video file from Supabase Storage to free up space.
        *   Uploads the compressed audio back to Supabase (for archive/playback).
    3.  **Transcription:** Use Whisper (v3 Large via API or Groq) to transcribe audio.
    4.  **Analysis:** LLM generates Summary, Key Insights, Sentiment, and Recommendation.
*   **Output:** Analysis JSON + Link to Audio File.

### **ADMIN & HISTORY**
*   **Log History:**
    *   Endpoint to fetch `activity_logs` joined with `profiles` (to see who did what).
    *   Visible to all users.
*   **Delete History:**
    *   Admin can delete a log entry.
    *   **Trigger:** When a log is deleted, Backend MUST also delete the associated physical files (`input_files`, `output_files`) from Supabase Storage to clean up.

---

## 6. DEVELOPMENT GUIDELINES FOR AI AGENT
1.  **Supabase Client:** Always use `SUPABASE_SERVICE_ROLE_KEY` in FastAPI to ensure backend has full access to Storage and DB without RLS issues during processing.
2.  **Async/Await:** All I/O operations (DB, AI API, Storage) must be asynchronous.
3.  **Error Handling:** If AI/PDDIKTI fails, do not crash. Return a structured error or "Manual Check Required" status.
4.  **File Paths:** Store only relative paths or full URLs in the database arrays. Consistency is key.
5.  **Clean Code:** Use `Services` pattern (e.g., `services/ai_service.py`, `services/storage_service.py`) to separate logic from `routers`.
```

***